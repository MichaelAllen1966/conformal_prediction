{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal prediction for classification\n",
    "\n",
    "This notebook will performal conformal prediction for classification manually, and also use the Python Package MAPIE (https://pypi.org/project/MAPIE/).\n",
    "\n",
    "If necessary, `pip install mapie`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mapie.classification import MapieClassifier\n",
    "from mapie.metrics import classification_coverage_score\n",
    "from mapie.metrics import classification_mean_width_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data\n",
    "\n",
    "Example data will be produced using SK-Learn's `make_blobs` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 4\n",
    "# Make train and test data\n",
    "X, y = make_blobs(\n",
    "    n_samples=10000, n_features=3, centers=n_classes, cluster_std=3.5, random_state=13)\n",
    "\n",
    "# Split off model training set\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "# Split rest into calibration and test\n",
    "X_Cal, X_test, y_cal, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig = plt.subplots(figsize=(5, 5))\n",
    "ax = plt.subplot(111)\n",
    "for i in range(n_classes):\n",
    "    ax.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], label=i, alpha=0.5, s=10)\n",
    "legend = ax.legend()\n",
    "legend.set_title(\"Class\")\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Manual' method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the classifier\n",
    "\n",
    "Use the training set to train a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buildand train the classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate conformal prediction threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now predict classiifcation probabilitlites of the calibration set. This will be used to set a classification threshold for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for calibration set\n",
    "y_pred = classifier.predict(X_Cal)\n",
    "y_pred_proba = classifier.predict_proba(X_Cal)\n",
    "\n",
    "# Show first 5 instances\n",
    "y_pred_proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate $s_i$ scores\n",
    "\n",
    "Here we will calculate $s_i$ scores only based on looking at probabilities associated with the observed class.\n",
    "\n",
    "For each instance we will get the predicted probability for the class of that instance.\n",
    "\n",
    "The $s_i$ score (non-conformality) is $1-probability$.\n",
    "\n",
    "The higher the $s_i$ score, the less that example conforms to that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_scores = []\n",
    "# Loop through all calibration instances\n",
    "for i, true_class in enumerate(y_cal):\n",
    "    # Get predicted probability for observed/true class\n",
    "    predicted_prob = y_pred_proba[i][true_class]\n",
    "    si_scores.append(1 - predicted_prob)    \n",
    "\n",
    "# Convert to NumPy array\n",
    "si_scores = np.array(si_scores)\n",
    "\n",
    "# Show first 5 instances\n",
    "si_scores[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 95th percentile threshold\n",
    "\n",
    "The threshold determines what *coverage* our classification will have. Coverage refers to the proportion of predictions that actually contain the true outcome. \n",
    "\n",
    "The threshold is the percentile corresponding to $1-alpha$. To get 95% coverage, we set an alpha of 0.05.\n",
    "\n",
    "When used in real life, the quantile level (based on ùõº) requires a finite sample correction to calculate the corresponding quantile ùëû. We multiple 0.95  by $(n+1)/n$, which means that ùëûùëôùëíùë£ùëíùëô would be 0.951 for n = 1000. We will ignore that correction here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = len(X_Cal)\n",
    "alpha = 0.05\n",
    "qlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\n",
    "threshold = np.percentile(si_scores, qlevel*100)\n",
    "print(f'Threshold: {threshold:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show chart of $s_1$ values, with cut-off threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(si_scores)) + 1\n",
    "sorted_si_scores = np.sort(si_scores)\n",
    "index_of_95th_percentile = int(len(si_scores) * 0.95)\n",
    "\n",
    "# Color by cut-off\n",
    "conform = 'g' * index_of_95th_percentile\n",
    "nonconform = 'r' * (len(si_scores) - index_of_95th_percentile)\n",
    "color = list(conform + nonconform)\n",
    "\n",
    "fig = plt.figure(figsize=((6,4)))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Add bars\n",
    "ax.bar(x, sorted_si_scores, width=1.0, color = color)\n",
    "\n",
    "# Add lines for 95th percentile\n",
    "ax.plot([0, index_of_95th_percentile],[threshold, threshold], \n",
    "        c='k', linestyle='--')\n",
    "ax.plot([index_of_95th_percentile, index_of_95th_percentile], [threshold, 0],\n",
    "        c='k', linestyle='--')\n",
    "\n",
    "# Add text\n",
    "txt = '95th percentile conformality threshold'\n",
    "ax.text(5, threshold + 0.04, txt)\n",
    "\n",
    "# Add axis labels\n",
    "ax.set_xlabel('Sample instance (sorted by $s_i$)')\n",
    "ax.set_ylabel('$S_i$ (non-conformality)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples/classes from test set classified as positive\n",
    "\n",
    "We can now find all those model outputs less than the threshold.\n",
    "\n",
    "It is possible for an individual example to have no value, or more than one value, below the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_sets = (1 - classifier.predict_proba(X_test) <= threshold)\n",
    "prediction_sets[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show coverage and average set size\n",
    "\n",
    "*Coverage* is the proportion of prediction sets that actually contain the true outcome.\n",
    "\n",
    "*Average set size* is the average number of predicted classes per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coverage and average set size\n",
    "cov = classification_coverage_score(y_test, prediction_sets)\n",
    "setsize = classification_mean_width_score(prediction_sets)\n",
    "print(f'Coverage: {cov:0.2f}')\n",
    "print(f'Avg. set size: {setsize:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show class-wise performance.\n",
    "\n",
    "NOTE: The standard scoring method guarantees coverage across the data set, but does not guarantee coverage for all classes. The method may be modified to use thresholds for each class (see end of the notebook for adaptations to the general method.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_wise_performance(y_new, y_set, n_classes):\n",
    "    df = pd.DataFrame()\n",
    "    # Loop through the classes\n",
    "    for i in range(n_classes):\n",
    "    # Calculate the coverage and set size for the current class\n",
    "        ynew = y_new[y_new == i]\n",
    "        yscore = y_set[y_new == i]\n",
    "        cov = classification_coverage_score(ynew, yscore)\n",
    "        size = classification_mean_width_score(yscore)\n",
    "        # Create a new dataframe with the calculated values\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"class\": [i],\n",
    "            \"coverage\": [cov],\n",
    "            \"avg. set size\": [size]\n",
    "            }, index = [i])\n",
    "        # Concatenate the new dataframe with the existing one\n",
    "        df = pd.concat([df, temp_df])\n",
    "    df.set_index('class', inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_wise_performance(y_test, prediction_sets, n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAPIE example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test sets\n",
    "\n",
    "We will use mapie's k-fold calibration/test method to get assessment of the whole test set, so there is no need for calibration/test here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and MAPIE classifier\n",
    "\n",
    "MAPIE wraps around any model. It fits the model and gets the prediction sets.\n",
    "\n",
    "The `score` method is the same as used above. This method may be used with k-fold MAPIE fitting which fits a model and then gives us prediction sets across the whole test set by using k-fold calibration/test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifier\n",
    "classifier = LogisticRegression()\n",
    "mapie_score = MapieClassifier(estimator=classifier, method='score', cv=5)\n",
    "# Fit classifier and get predictions\n",
    "mapie_score.fit(X_train, y_train)\n",
    "y_pred, y_set = mapie_score.predict(X_test, alpha=0.05)\n",
    "# Remove redundant dimension (used if more than one alpha is used)\n",
    "y_set = np.squeeze(y_set)\n",
    "# Show first 5 instances\n",
    "print(y_set[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full coverage\n",
    "cov = classification_coverage_score(y_test, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "print(f'Coverage: {cov:0.2f}')\n",
    "print(f'Avg. set size: {setsize:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class-wise performance\n",
    "print('Class wise performance')\n",
    "print(class_wise_performance(y_test, y_set, n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method adaptations\n",
    "\n",
    "The method may be adapted in various ways:\n",
    "\n",
    "- *Adaptive methods* can be better when some examples are easier to classify than others, or when there is easy confusion between groups (tigers vs lions, for example). These can produce large sets, but RAPS (Regualirised Adaptive Prediction Sets) includes a penalty to reduce this problem.\n",
    "\n",
    "- *Group-balanced* conformal prediction guarantees coverage for groups in the data. If there are separate groups where it is essential to get equal coverage (e.g. race), then calculate thresholds for each group separately. This method relies on having sufficent data for reliable subgroups.\n",
    "\n",
    "- *Class-conditional* conformal prediction guarantees coverage for all classes. This uses class as the grouping method. This can tend to increase set size as coverage must be for all classes, and the the most difficult class to classify may be 'triggered' most.\n",
    "\n",
    "- *Top K* gives fixed size prediction sets. It selects $k$ instances that have the lowest non-conformity score.\n",
    "\n",
    "Some of these methods require prefitting of a model with calibration, rather than being able to use MAPIE's k-fold method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
